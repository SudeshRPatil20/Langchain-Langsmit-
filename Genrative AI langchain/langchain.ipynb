{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3591e5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sudesh\\Gerative AI\\Genrative AI langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "os.environ['GOOGLE_API_KEY']=os.getenv('GOOGLE_API_KEY')\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRAINCING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_PROJECT']=os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f91bc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001 -> ['generateMessage', 'countMessageTokens']\n",
      "models/text-bison-001 -> ['generateText', 'countTextTokens', 'createTunedTextModel']\n",
      "models/embedding-gecko-001 -> ['embedText', 'countTextTokens']\n",
      "models/gemini-1.0-pro-vision-latest -> ['generateContent', 'countTokens']\n",
      "models/gemini-pro-vision -> ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-latest -> ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-001 -> ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro-002 -> ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro -> ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-latest -> ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-001 -> ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-001-tuning -> ['generateContent', 'countTokens', 'createTunedModel']\n",
      "models/gemini-1.5-flash -> ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-002 -> ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-8b -> ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-001 -> ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-latest -> ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-exp-0827 -> ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-exp-0924 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-pro-exp-03-25 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-pro-preview-03-25 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-exp -> ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash -> ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-001 -> ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-exp-image-generation -> ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-lite-001 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite-preview-02-05 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite-preview -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-pro-exp -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-pro-exp-02-05 -> ['generateContent', 'countTokens']\n",
      "models/gemini-exp-1206 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-thinking-exp-01-21 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-thinking-exp -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-thinking-exp-1219 -> ['generateContent', 'countTokens']\n",
      "models/learnlm-1.5-pro-experimental -> ['generateContent', 'countTokens']\n",
      "models/gemma-3-1b-it -> ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it -> ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it -> ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it -> ['generateContent', 'countTokens']\n",
      "models/embedding-001 -> ['embedContent']\n",
      "models/text-embedding-004 -> ['embedContent']\n",
      "models/gemini-embedding-exp-03-07 -> ['embedContent', 'countTextTokens']\n",
      "models/gemini-embedding-exp -> ['embedContent', 'countTextTokens']\n",
      "models/aqa -> ['generateAnswer']\n",
      "models/imagen-3.0-generate-002 -> ['predict']\n",
      "models/gemini-2.0-flash-live-001 -> ['bidiGenerateContent', 'countTokens']\n"
     ]
    }
   ],
   "source": [
    "# list of gemmini models that are used by api\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyBZQovVczOfkWSfA6fD9HqxhIBFNT3edhU\")\n",
    "\n",
    "models = genai.list_models()\n",
    "for model in models:\n",
    "    print(model.name, \"->\", model.supported_generation_methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568e8059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-pro', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002B9F4D76CE0>, default_metadata=())"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a5516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine a regular computer bit like a light switch: it can be either ON (1) or OFF (0).  A quantum bit, or qubit, is like a special light switch that can be ON, OFF, or *both at the same time*. This \"both at the same time\" state is called superposition.\n",
      "\n",
      "Think of it like flipping a coin: while it's spinning in the air, it's both heads and tails *potentially*.  Only when it lands does it become definitely one or the other. Qubits are similar â€“ they exist in a superposition of states until they are measured.\n",
      "\n",
      "Another key idea is entanglement.  This is like having two of our special light switches linked together magically. If one is ON, the other is instantly OFF, no matter how far apart they are.  This connection lets quantum computers perform calculations in a way that regular computers can't.\n",
      "\n",
      "Because qubits can be in multiple states at once, a quantum computer can explore many possibilities simultaneously.  This makes them incredibly powerful for certain types of problems, like:\n",
      "\n",
      "* **Drug discovery:** Simulating molecules to design new drugs.\n",
      "* **Materials science:** Creating new materials with specific properties.\n",
      "* **Financial modeling:** Predicting market trends and managing risk.\n",
      "* **Cryptography:** Breaking current encryption methods and creating new ones.\n",
      "\n",
      "However, quantum computers are still in their early stages of development.  They are very complex and difficult to build and control.  They won't replace our regular computers anytime soon, but they have the potential to revolutionize certain fields in the future.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test prompt\n",
    "response = llm.invoke(\"Explain quantum computing in simple terms.\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a021af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langchain) (0.3.52)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langchain) (0.3.31)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langchain) (2.11.3)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.40-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.2.0-cp310-cp310-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: anyio in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\sudesh\\gerative ai\\genrative ai langchain\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Using cached langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.0-cp310-cp310-win_amd64.whl (294 kB)\n",
      "Installing collected packages: greenlet, async-timeout, SQLAlchemy, langchain-text-splitters, langchain\n",
      "Successfully installed SQLAlchemy-2.0.40 async-timeout-4.0.3 greenlet-3.2.0 langchain-0.3.23 langchain-text-splitters-0.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402039c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001 -> ['generateMessage', 'countMessageTokens']\n",
      "models/text-bison-001 -> ['generateText', 'countTextTokens', 'createTunedTextModel']\n",
      "models/embedding-gecko-001 -> ['embedText', 'countTextTokens']\n",
      "models/gemini-1.0-pro-vision-latest -> ['generateContent', 'countTokens']\n",
      "models/gemini-pro-vision -> ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-latest -> ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-001 -> ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro-002 -> ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro -> ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-latest -> ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-001 -> ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-001-tuning -> ['generateContent', 'countTokens', 'createTunedModel']\n",
      "models/gemini-1.5-flash -> ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-002 -> ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-8b -> ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-001 -> ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-latest -> ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-exp-0827 -> ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-exp-0924 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-pro-exp-03-25 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-pro-preview-03-25 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-exp -> ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash -> ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-001 -> ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-exp-image-generation -> ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-lite-001 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite-preview-02-05 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite-preview -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-pro-exp -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-pro-exp-02-05 -> ['generateContent', 'countTokens']\n",
      "models/gemini-exp-1206 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-thinking-exp-01-21 -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-thinking-exp -> ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-thinking-exp-1219 -> ['generateContent', 'countTokens']\n",
      "models/learnlm-1.5-pro-experimental -> ['generateContent', 'countTokens']\n",
      "models/gemma-3-1b-it -> ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it -> ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it -> ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it -> ['generateContent', 'countTokens']\n",
      "models/embedding-001 -> ['embedContent']\n",
      "models/text-embedding-004 -> ['embedContent']\n",
      "models/gemini-embedding-exp-03-07 -> ['embedContent', 'countTextTokens']\n",
      "models/gemini-embedding-exp -> ['embedContent', 'countTextTokens']\n",
      "models/aqa -> ['generateAnswer']\n",
      "models/imagen-3.0-generate-002 -> ['predict']\n",
      "models/gemini-2.0-flash-live-001 -> ['bidiGenerateContent', 'countTokens']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec23ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='your expected AI engineer. Provide me answers on the bases of question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating chatprompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompts=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"your expected AI engineer. Provide me answers on the bases of question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc65842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\"GenAI\" is shorthand for **Generative AI**.  It refers to a category of artificial intelligence algorithms designed to generate new content, ranging from text and code to images, audio, video, and 3D models.  Instead of simply analyzing or classifying existing data, generative AI *creates* something new based on the data it has been trained on.\\n\\nHere\\'s a breakdown of key aspects of GenAI:\\n\\n* **How it works:** GenAI models learn the underlying patterns and structure of their training data and then use this knowledge to generate similar but novel outputs.  Many GenAI models are based on deep learning architectures like Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformers.\\n\\n* **Types of content it generates:**\\n    * **Text:**  Poems, code, scripts, musical pieces, emails, letters, etc. (e.g., ChatGPT, Bard)\\n    * **Images:**  Photos, artwork, sketches, logos, etc. (e.g., DALL-E 2, Midjourney, Stable Diffusion)\\n    * **Audio:**  Music, sound effects, voiceovers, etc.\\n    * **Video:**  Short films, animations, special effects, etc.\\n    * **3D Models:**  Objects, characters, environments, etc.\\n\\n* **Key applications:**\\n    * **Creative content generation:**  Assisting artists, writers, musicians, and designers.\\n    * **Code generation and software development:** Automating coding tasks and creating new software.\\n    * **Drug discovery and materials science:** Designing new molecules and materials with desired properties.\\n    * **Personalized marketing and advertising:** Creating targeted content for individual consumers.\\n    * **Synthetic data generation:**  Creating realistic synthetic data for training other AI models or for privacy-preserving applications.\\n\\n* **Challenges and limitations:**\\n    * **Bias:** GenAI models can inherit and amplify biases present in their training data, leading to unfair or discriminatory outputs.\\n    * **Control and interpretability:**  It can be difficult to control the output of GenAI models and understand how they arrive at their results.\\n    * **Misinformation and misuse:** GenAI can be used to create deepfakes and other forms of misinformation, raising ethical concerns.\\n    * **Computational resources:** Training and running large GenAI models requires significant computational power and energy.\\n\\n\\nIn essence, GenAI is a powerful and rapidly evolving field with the potential to revolutionize many industries and aspects of our lives. However, it\\'s crucial to be aware of its limitations and potential risks to ensure responsible development and deployment.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-5887531f-6398-48a6-999d-6b8778079539-0' usage_metadata={'input_tokens': 18, 'output_tokens': 548, 'total_tokens': 566, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain=prompts|llm\n",
    "\n",
    "response=chain.invoke(\"what is genai?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94773d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"GenAI\" is shorthand for **Generative Artificial Intelligence**.  It refers to a category of artificial intelligence algorithms that are capable of creating new content, ranging from text and code to images, music, and even 3D models.  Unlike other AI systems that are designed to classify or analyze existing data, generative AI models learn the underlying patterns and structure of their input training data and then generate similar, but novel, outputs.\n",
      "\n",
      "Here's a breakdown of key aspects of GenAI:\n",
      "\n",
      "* **How it works:**  GenAI models typically use deep learning methods, particularly neural networks like Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformers. These models are trained on massive datasets and learn to generate outputs that resemble the training data in terms of style, structure, and content.\n",
      "* **Types of content generated:** GenAI can create various types of content, including:\n",
      "    * **Text:**  Articles, poems, scripts, code, summaries, translations\n",
      "    * **Images:**  Photos, artwork, illustrations, logos\n",
      "    * **Audio:**  Music, sound effects, voiceovers\n",
      "    * **Video:**  Short clips, animations\n",
      "    * **3D Models:**  Objects, environments, characters\n",
      "* **Key applications:** GenAI has numerous applications across various industries:\n",
      "    * **Creative content generation:**  Assisting artists, writers, and musicians in creating new works.\n",
      "    * **Code generation:** Automating coding tasks and assisting software developers.\n",
      "    * **Design and prototyping:**  Generating design ideas and creating prototypes for products.\n",
      "    * **Marketing and advertising:**  Creating personalized content and generating marketing materials.\n",
      "    * **Drug discovery and development:** Designing new molecules and predicting their properties.\n",
      "* **Examples of GenAI models:**\n",
      "    * **GPT-3 and GPT-4 (text):**  Large language models capable of generating human-quality text.\n",
      "    * **DALL-E 2 and Stable Diffusion (images):**  Models that generate images from text descriptions.\n",
      "    * **Midjourney (images):**  AI art generator creating images from text prompts.\n",
      "    * **GitHub Copilot (code):**  AI pair programmer that suggests code completions and generates code.\n",
      "    * **MuseNet and Jukebox (music):**  Models that generate music in various styles.\n",
      "\n",
      "\n",
      "In essence, GenAI is a powerful tool that can automate creative tasks, enhance human creativity, and drive innovation across various fields.  However, it also raises important ethical considerations regarding copyright, misuse, and the potential displacement of human creators.\n"
     ]
    }
   ],
   "source": [
    "# string output parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompts|llm|output_parser #its basically aprocess chain how it will go\n",
    "\n",
    "response=chain.invoke(\"what is genai?\")\n",
    "print(response) #as you can see it is showing directly answer with out extra content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d83ed691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n",
      "Trace URL: https://smith.langchain.com/o/8eadc964-999e-4ec9-b5b3-a39cac4e3840/projects/p/703ed294-2332-4fcc-b8cc-97ceda5f0972/r/4020c301-0c2e-4176-8d1c-e2e58d9d700a?poll=true\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import tracing_v2_enabled\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "with tracing_v2_enabled(project_name=\"GenAIAPPWithGEMMINAI\") as tracer:\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "    response = llm.invoke(\"Hello!\")\n",
    "    print(response.content)\n",
    "    trace_url = tracer.get_run_url()\n",
    "    print(\"Trace URL:\", trace_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cadf42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
